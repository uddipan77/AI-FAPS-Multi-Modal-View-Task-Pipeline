# Example config for your new MultiTaskModule
_target_: src.models.multitask_module.MultiTaskModule

modal_nets:
  images:
    _target_: src.models.components.torchvision_wrapper.TorchVisionWrapper
    model_name: densenet121
    weights: DEFAULT
    unfreeze_layer_count: 100

  forces:
    _target_: tsai.models.ResCNN.ResCNN #InceptionTime.InceptionTime
    c_in: 5
    c_out: 32  #if we change this then self.fused_feature_dim = 1000 + c_out, so we would need to change that as well
    #hidden_size: 211 #for gru
    #n_layers: 1      #for gru
    #rnn_dropout: 0.06417718142436428     #for gru
    #bidirectional: false                 #for gru
    #fc_dropout: 0.0001036446690224832    #for gru


    #nf: 32             #for inception time and inceptiontimeplus
    #depth: 8           #for inception time and inceptiontimeplus
    #bottleneck: true   #for inception time and inceptiontimeplus
    coord: True        #for rescnn
    separable: True    #for rescnn
    zero_norm: false   #for rescnn
    



fusion_type: concat
num_classes: 2

# weighting for each task
alpha_classification: 1.0
alpha_regression: 1.0

weight_f1: 0.5
weight_r2: 0.5

optimizer_cfg:
  _target_: torch.optim.Adam
  _partial_: true  ## ensures Hydra won't call AdamW(...) now but it will be called in my class during runtime
  lr: 0.001 #0.0017978167203413753
  weight_decay: 0.0009377397108070718 #1.2008655242742152e-09

scheduler_cfg:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.30328036943732606 #0.8358284602577417 
  patience: 5
  
