# Example config for your new MultiTaskModule
_target_: src.models.multitask_concat.MultiTaskModule

modal_nets:
  images:
    _target_: src.models.components.torchvision_wrapper.TorchVisionWrapper #src.models.components.dino_v2_wrapper.DinoV2Wrapper #src.models.components.torchvision_wrapper.TorchVisionWrapper
    model_name: densenet121
    weights: DEFAULT
    unfreeze_layer_count: 100
    #dropout: 0.2

  forces:
    _target_: tsai.models.RNN.GRU #XResNet1d.xresnet1d50_deep #InceptionTimePlus.InceptionTimePlus  #InceptionTime.InceptionTime
    c_in: 5
    c_out: 64  #if we change this then self.fused_feature_dim = 1000 + c_out, so we would need to change that as well
    hidden_size: 211 #for gru
    n_layers: 1     #for gru
    rnn_dropout:  0.06417718142436428     #for gru
    bidirectional: false                 #for gru
    fc_dropout: 0.0001036446690224832   #for gru


    #nf: 64             #for inception time and inceptiontimeplus
    #depth: 8          #for inception time and inceptiontimeplus
    #bottleneck: True   #for inception time and inceptiontimeplus
    #coord: True        #for rescnn
    #separable: False    #for rescnn
    #zero_norm: True   #for rescnn
    



fusion_type: concat
num_classes: 2

# weighting for each task
alpha_classification: 1.0
alpha_regression: 1.0

weight_f1: 1
weight_r2: 1

optimizer_cfg:
  _target_: torch.optim.Adam
  _partial_: true  ## ensures Hydra won't call AdamW(...) now but it will be called in my class during runtime
  lr: 0.001 #0.0017978167203413753  #0.0017978167203413753  
  weight_decay: 4.8111902716217195e-08   #0.0009377397108070718  #1.2008655242742152e-09

scheduler_cfg:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor:  0.5 #0.30328036943732606 #0.8358284602577417 
  patience: 5
  
