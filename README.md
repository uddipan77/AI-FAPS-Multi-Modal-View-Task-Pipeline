<div align="center">

# Deep Multimodal Multiview Multitask Learning for Process Monitoring in Industrial Manufacturing

[![python](https://img.shields.io/badge/-Python_3.11-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![lightning](https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/)
[![wandb](https://img.shields.io/badge/Weights_&_Biases-FFCC33?logo=WeightsAndBiases&logoColor=black)](https://wandb.ai/site)

A flexible and configurable pipeline designed to train and evaluate a range of machine learning models on specific downstream tasks for multimodal datasets.

</div>

<br>

### 📝 Table of Contents

- [Deep Multimodal Learning for Process Monitoring in Industrial Manufacturing](#deep-multimodal-learning-for-process-monitoring-in-industrial-manufacturing)
    - [📝 Table of Contents](#-table-of-contents)
    - [📌 Introduction](#-introduction)
    - [📦 Built With](#-built-with)
    - [📂 Project Structure](#-project-structure)
    - [🚀  Setup](#setup)
    - [🗃️ Data](#️-data)
    - [🤖  Training](#training)
    - [🧪  Evaluation](#evaluation)
    - [📈 Weight and Biases Logging](#-weight-and-biases-logging)
    - [Bonus](#bonus)
      - [⏱️ Schedule a Job on HPC](#️-schedule-a-job-on-hpc)
      - [Useful VSCode Extensions](#useful-vscode-extensions)
    - [📬 Contact](#-contact)
    - [📚 References](#-references)


### 📌 Introduction

This repository contains the code for evaluation of different Deep Multimodal techniques for Process Monitoring in Industrial Manufacturing.

<br>

### 📦 Built With

[PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) - a lightweight PyTorch wrapper for high-performance AI research. Think of it as a framework for organizing your PyTorch code.

[Hydra](https://github.com/facebookresearch/hydra) - a framework for elegantly configuring complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line.

[Weights and Biases](https://wandb.ai/site) - a tool for tracking and visualizing machine learning experiments.

[Visual Studio Code](https://code.visualstudio.com/) - a code editor redefined and optimized for building applications.

[FAU High Performance Computing](https://doc.nhr.fau.de/) - a high-performance computing cluster at Friedrich-Alexander-Universität Erlangen-Nürnberg.

<br>

### 📂 Project Structure

The directory structure of the project looks like this:

```
├── .github                   <- Github Actions workflows
│
├── configs                   <- Hydra configs
│   ├── callbacks                <- Callbacks configs
│   ├── data                     <- Data configs
│   ├── debug                    <- Debugging configs
│   ├── experiment               <- Experiment configs
│   ├── extras                   <- Extra utilities configs
│   ├── hparams_search           <- Hyperparameter search configs
│   ├── hyperparameter_search    <- Hyperparameter search configs
│   ├── hydra                    <- Hydra configs
│   ├── local                    <- Local configs
│   ├── logger                   <- Logger configs
│   ├── model                    <- Model configs
│   ├── paths                    <- Project paths configs
│   ├── trainer                  <- Trainer configs
│   │
│   ├── eval.yaml             <- Main config for evaluation
│   └── train.yaml            <- Main config for training
│
├── data                   <- Project data
│
├── EDA                    <- Exploratory Data Analysis ipynb files
│
├── logs                   <- Logs generated by hydra and lightning loggers
│   ├── train                    <- Training logs and checkpoints
│   ├── debug                    <- Debugging logs
│   ├── hpc                      <- HPC job logs
│   ├── lightning_logs           <- PyTorch Lightning logs
│   ├── optuna                   <- Optuna hyperparameter optimization logs
│   └── eval                     <- Evaluation logs
│
├── scripts                <- Shell scripts
│
├── src                    <- Source code
│   ├── __pycache__              <- Python cache directory
│   ├── data                     <- Data scripts
│   ├── models                   <- Model scripts
│   │   ├── __pycache__              <- Python cache directory
│   │   ├── components               <- Model components directory
│   │   ├── __init__.py              <- Makes models a Python package
│   │   ├── GRU_regressionmodel.py   <- GRU regression model implementation
│   │   ├── InceptionTime_regressionmodel.py    <- InceptionTime regression model
│   │   ├── InceptionTimePlus_regressionmodel.py <- InceptionTimePlus model
│   │   ├── mnist_module.py          <- MNIST module implementation
│   │   ├── multimodal_module.py     <- Multimodal multiview module implementation
│   │   ├── multitask_concat.py      <- Multimodal multiview multitask with image features concatenation implementation
│   │   ├── multitask_mean.py        <- Multimodal multiview multitask with image features mean implementation
│   │   ├── multitask_module.py      <- Multitask module implementation
│   │   ├── regression_module.py     <- Regression module implementation
│   │   └── XResNet1d_regressionmodel.py <- XResNet1d unimodal regression model
│   │
│   ├── utils                    <- Utility scripts
│   │
│   ├── __init__.py              <- Makes src a Python package
│   ├── eval.py                  <- Run evaluation
│   ├── train.py                 <- Run training
│   ├── train_cv.py              <- Cross-validation training script
│   ├── train_optuna.py          <- Optuna hyperparameter optimization script
│   ├── train_optuna2.py         <- Second Optuna optimization script
│   ├── train_optuna3.py         <- Third Optuna optimization script
│   ├── train_optuna4.py         <- Fourth Optuna optimization script
│   └── train.log                <- Training log file
│
├── tests                  <- Tests of any kind
│
├── .env.example              <- Example of file for storing private environment variables
├── .gitignore                <- List of files ignored by git
├── .pre-commit-config.yaml   <- Configuration of pre-commit hooks for code formatting
├── .project-root             <- File for inferring the position of project root directory
├── environment.yaml          <- File for installing conda environment
├── Makefile                  <- Makefile with commands like `make train` or `make test`
├── pyproject.toml            <- Configuration options for testing and linting
├── requirements.txt          <- File for installing python dependencies
├── setup.py                  <- File for installing project as a package
└── README.md
```

<br>

### 🚀  Setup

```bash
# create conda environment
conda env create -f environment.yml
conda activate faps
```
</div>

<br>

### 🗃️ Data

The data should be stored in the data directory or a symbolic soft link can be created to target the dataset in this directory. Using symbolic soft links is recommended to avoid duplication of large datasets and to separate the storage locations for code and data.

For example, the linear winding dataset can be linked in the `data` directory as follows:

```bash
cd data
mkdir LinearWinding && cd LinearWinding
ln -s /absolute/path/to/linear_winding_dataset dataset_linear_winding_multimodal
```

This will create a symbolic link to the dataset in the `data/LinearWinding` directory with the name `dataset_linear_winding_multimodal`.

<br>

### 🤖  Training

<details>
<summary>
Default example experiment of the pipeline
</summary>

```bash
# Uses the default experiment and configuration to train the model
# The default experiment is MNIST classification with a simple CNN
python src/train.py
```
</details>

<details>
<summary>
Specific experiment with local CSV logger and GPU accelerator
</summary>

```bash
# Use experiments from configs/experiment directory to train model
python src/train.py experiment=multimodal/image_force/densenet_cnn1d logger=csv trainer=gpu
```
</details>

<details>
<summary>
Specific experiment with Weights and Biases logger and GPU accelerator <sup>✨ Most used</sup>
</summary>

```bash
# Use experiments from configs/experiment directory to train model
python src/train.py experiment=multimodal/image_force/densenet_cnn1d logger=wandb trainer=gpu
```
</details>


<details>
<summary>
Check execution of specific experiment using CPU accelerator
</summary>

```bash
# Used for quick debugging and testing
python src/train.py experiment=multimodal/image_force/densenet_cnn1d trainer=cpu trainer.max_epochs=1
```
</details>

<details>
<summary>
Run in debug mode
</summary>

```bash
# runs 1 epoch in default debugging mode
# changes logging directory to `logs/debugs/...`
# sets level of all command line loggers to 'DEBUG'
# enforces debug-friendly configuration
python src/train.py experiment=multimodal/image_force/densenet_cnn1d debug=default

# run 1 train, val and test loop, using only 1 batch
python src/train.py experiment=multimodal/image_force/densenet_cnn1d debug=fdr

# print execution time profiling
python src/train.py experiment=multimodal/image_force/densenet_cnn1d debug=profiler

# try overfitting to 1 batch
python src/train.py experiment=multimodal/image_force/densenet_cnn1d debug=overfit

# raise exception if there are any numerical anomalies in tensors, like NaN or +/-inf
python src/train.py experiment=multimodal/image_force/densenet_cnn1d +trainer.detect_anomaly=true

# use only 20% of the data
python src/train.py experiment=multimodal/image_force/densenet_cnn1d +trainer.limit_train_batches=0.2 \
+trainer.limit_val_batches=0.2 +trainer.limit_test_batches=0.2
```
</details>

<br>

### 🧪  Evaluation

<details>
<summary>
Test the model using the default configuration and a checkpoint
</summary>

```bash
# Evaluate the model using the default configuration and checkpoint
python src/eval.py ckpt_path="/path/to/ckpt/name.ckpt"

# You need to give the absolute path to checkpoint file
# For Example:
python src/eval.py ckpt_path="/home/vault/iwfa/iwfa110h/runs_uddipan2/epoch_009.ckpt"
```
</details>

<br>

### 📈 Weight and Biases Logging

Steps to use Weights and Biases for logging and visualization:

1. Create an account on [Weights and Biases](https://wandb.ai/site).
2. Install the Weights and Biases library using `pip install wandb`.
3. Login to your account using `wandb login`.
4. Update the `wandb` section in the `configs/logger/wandb.yaml` file with your project name and entity.
5. Run the training script with the `logger=wandb` argument.

<br>

### Bonus

#### ⏱️ Schedule a Job on HPC

For interactive scheduling a job on the HPC, you can use the `scripts/schedule.py` script. The script is designed to interactively schedule the training script on the HPC with the specified configuration.

```bash
# Schedule a job on the HPC interactively
cd scripts && python schedule.py
```

#### Useful VSCode Extensions

- [Remote Explorer](https://marketplace.visualstudio.com/items?itemName=ms-vscode.remote-explorer) - Open projects on remote servers.
- [Log Viewer](https://marketplace.visualstudio.com/items?itemName=berublan.vscode-log-viewer) - A log monitoring extension.
- [Black Formatter](https://marketplace.visualstudio.com/items?itemName=ms-python.black-formatter) - Python auto code formatter.
- [Markdown All in One](https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one) - Markdown preview and editing.

<br>

### 📬 Contact

Uddipan Basu Bir - uddipan.bir.basu@fau.de - uddipanbb95@gmail.com

<br>

### 📚 References

Boilerplate code and project structure inspired by [Lightning Hydra Template](https://github.com/ashleve/lightning-hydra-template).

<br>